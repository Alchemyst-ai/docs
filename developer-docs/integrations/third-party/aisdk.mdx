---
title: 'Vercel AI SDK'
description: "Integrate Alchemyst's memory with Vercel AI SDK"
icon: "js"
keywords: ['ai-sdk', 'vercel', 'integration']
---
## About Vercel AI SDK
[Vercel AI SDK](https://ai-sdk.dev) is an open-source toolkit from the team behind Vercel and Next.js. It provides a **unified developer experience** for building AI-powered applications, including:

- Easy integration with multiple model providers (OpenAI, Anthropic, etc.)
- Support for **streaming responses** for real-time chat UIs
- Type-safe APIs with excellent **TypeScript support**
- Built-in support for **agents, tools, and structured outputs**
- Ready-to-use React/Next.js hooks for managing conversation state

By using the AI SDK, you can avoid handling low-level APIs directly and focus on creating seamless AI-driven experiences in your app.

---

## How It Works

The AI SDK is split into modular packages:

- **Core API**: A unified way to call LLMs and handle outputs
- **Provider Adapters**: Packages like `@ai-sdk/openai` let you plug in specific providers
- **UI Utilities**: Hooks such as `useChat` make it easy to build interactive experiences
- **Tooling / Agents**: Support for calling external APIs or chaining workflows

This modular design means you can start small and scale up to more complex AI flows as your application grows.

## When to Use Vercel AI SDK

You should consider using the SDK if:
- You want to build chatbots, assistants, or agent-like applications quickly
- You need real-time streaming responses from your models
- You want cross-provider flexibility without rewriting core logic
- You prefer type safety and well-structured APIs

For more details, check out the official [**Vercel AI SDK documentation**](https://ai-sdk.dev/docs/introduction)

<Tabs>

<Tab title="As AI-SDK Tool" icon="code">
## As a Tool in AI-SDK
Alchemyst can be added to your AI SDK codebase as a regular tool as well, which is the recommended way for developer control.

The snippet below shows how to set up a tool using Vercel's AI SDK, using OpenAI GPT-5-Nano. This tool uses [**Alchemyst AI SDK**](https://npmjs.com/package/@alchemystai/sdk) under the hood, and exposes a nifty set of tools, letting you control if you want to use memory, context or both (default).

This assumes that you have an OpenAI Key and Alchemyst API Key. If you don't have the Alchemyst API Key, you can get them in the [**Alchemyst Settings page**](https://platform.getalchemystai.com/settings)

### Basic Setup

```ts aiSdkToolSetup.ts
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { alchemystTools } from '@alchemystai/aisdk';

const result = await streamText({
  model: openai('gpt-4o-mini'),
  prompt: "Search for users with the name rohan",
  tools: alchemystTools("YOUR_ALCHEMYST_AI_KEY", true, true)
});
```

### Complete Streaming Example

This example shows how to handle streaming responses and tool calls:

```ts streamingExample.ts
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { alchemystTools } from '@alchemystai/aisdk';


const result = await streamText({
  model: openai('gpt-4o-mini'),
  prompt: "What information do we have about our product documentation?",
  tools: alchemystTools("YOUR_ALCHEMYST_AI_KEY", true, true)
});

// Stream the text response
for await (const chunk of result.textStream) {
  process.stdout.write(chunk);
}

// Handle tool calls if any were made
const toolCalls = await result.toolCalls;
if (toolCalls && toolCalls.length > 0) {
  console.log('\n\nTool calls made:');
  for (const toolCall of toolCalls) {
    console.log(`- ${toolCall.toolName}:`, toolCall.args);
  }
}

// Get the full text result
const fullText = await result.text;
console.log('\n\nFull response:', fullText);
```

### Context Search Example

This example demonstrates how the AI can search your stored context:

```ts contextSearchExample.ts
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { alchemystTools } from '@alchemystai/aisdk';

const result = await streamText({
  model: openai('gpt-4o-mini'),
  prompt: "Find all information about our pricing plans and feature comparisons",
  tools: alchemystTools("YOUR_ALCHEMYST_AI_KEY", true, true)
});

// The AI will automatically use the search_context tool to find relevant information
for await (const chunk of result.textStream) {
  process.stdout.write(chunk);
}
```

### Adding Context Example

This example shows how to add context that the AI can later search:

```ts addContextExample.ts
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { alchemystTools } from '@alchemystai/aisdk';

// First, add some context
const addResult = await streamText({
  model: openai('gpt-4o-mini'),
  prompt: `store this information:  rohan is a good boy`,
  tools: alchemystTools("YOUR_ALCHEMYST_AI_KEY", true, true)
});


const toolCalls = await addResult.toolCalls;
if (toolCalls && toolCalls.length > 0) {
  console.log('\n\nTool calls made:');
  for (const toolCall of toolCalls) {
    console.log(`- ${toolCall.toolName}:`, JSON.stringify(toolCall, null, 2));
  }
}
// Wait for the tool to execute
await addResult.text;

// Now search for it
const searchResult = await streamText({
  model: openai('gpt-4o-mini'),
  prompt: "Search for users with the name rohan",
  tools: alchemystTools("YOUR_ALCHEMYST_AI_KEY", true, true)
});

for await (const chunk of searchResult.textStream) {
  process.stdout.write(chunk);
}
```

### Memory Management Example

This example demonstrates using memory to store user preferences:

```ts memoryExample.ts
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { alchemystTools } from '@alchemystai/aisdk';

const userId = "user_123";
const memoryId = `memory_${userId}`;

// Store user preferences in memory
const storeMemory = await streamText({
  model: openai('gpt-4o-mini'),
  prompt: `Remember that user ${userId} prefers:
    - Dark mode interface
    - Email notifications disabled
    - Preferred language: English
    - Timezone: IST`,
  tools: alchemystTools("YOUR_ALCHEMYST_AI_KEY", false, true) // Only memory, no context
});

const toolCalls = await storeMemory.toolCalls;
if (toolCalls && toolCalls.length > 0) {
  console.log('\n\nTool calls made:');
  for (const toolCall of toolCalls) {
    console.log(`- ${toolCall.toolName}:`, JSON.stringify(toolCall, null, 2));
  }
}

await storeMemory.text;

// Later, retrieve and use the memory
const retrieveMemory = await streamText({
  model: openai('gpt-4o-mini'),
  prompt: `What are the preferences for user ${userId}?`,
  tools: alchemystTools("YOUR_ALCHEMYST_AI_KEY", false, true)
});

for await (const chunk of retrieveMemory.textStream) {
  process.stdout.write(chunk);
}
```

### Chat Assistant with Context and Memory

A complete chat assistant that uses both context and memory:

```ts chatAssistant.ts
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { alchemystTools } from '@alchemystai/aisdk';

async function chatWithAssistant(userMessage: string, userId: string) {
  const result = await streamText({
    model: openai('gpt-4o-mini'),
    messages: [
      {
        role: 'system',
        content: 'You are a helpful assistant with access to company knowledge and user preferences.'
      },
      {
        role: 'user',
        content: userMessage
      }
    ],
    tools: alchemystTools("YOUR_ALCHEMYST_AI_KEY", true, true) // Both context and memory
  });

  // Stream the response
  let response = '';
  for await (const chunk of result.textStream) {
    process.stdout.write(chunk);
    response += chunk;
  }

  // Check for tool calls
  const toolCalls = await result.toolCalls;
  if (toolCalls) {
    console.log('\n\n[Tools used:', toolCalls.map(t => t.toolName).join(', '), ']');
  }

  return response;
}

// Example usage
await chatWithAssistant("who is rohan?", "user_123");
```

### Tool Configuration Options

Control which tools are available:

```ts toolConfiguration.ts
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { alchemystTools } from '@alchemystai/aisdk';

// Only context tools (search, add, delete context)
const contextOnly = alchemystTools("YOUR_ALCHEMYST_AI_KEY", true, false);

// Only memory tools (add, delete memory)
const memoryOnly = alchemystTools("YOUR_ALCHEMYST_AI_KEY", false, true);

// Both context and memory (default)
const both = alchemystTools("YOUR_ALCHEMYST_AI_KEY", true, true);

// Use context-only for knowledge base queries
const result = await streamText({
  model: openai('gpt-4o-mini'),
  prompt: "Search our documentation for API examples",
  tools: contextOnly
});
```
</Tab>

<Tab title="As MCP Tool (experimental)" icon="plug">
## MCP Client Setup
Alchemyst can be added to your AI SDK codebase as an MCP Tool, which is (probably) the most convenient way to do so.

The snippet below shows how to set up an **MCP (Model Context Protocol) Client** using Vercel's AI SDK. This client communicates with your server via SSE (Server-Sent Events) and can be configured with custom headers for authentication.

```ts aiSdkMcpSetup.ts
import { experimental_createMCPClient as createMCPClient } from 'ai';

const mcpClient = await createMCPClient({
  transport: {
    type: 'sse',
    url: 'https://mcp.getalchemystai.com/sse',

    // Configure HTTP headers, e.g. for authentication
    headers: {
      Authorization: 'Bearer YOUR_ALCHEMYST_AI_API_KEY',
    },
  },
});
```
</Tab>

</Tabs>

