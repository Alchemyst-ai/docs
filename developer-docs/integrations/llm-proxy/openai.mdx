---
title: 'OpenAI'
description: "Proxy server for routing requests to OpenAI LLM API"
---

## Why use the proxy server ?

By simply changing the `baseURL`, you can instantly add a memory layer to your LLM calls without altering your existing code. The proxy automatically handles storing and retrieving context across conversations, making your LLM smarter and more context-aware. Instead of managing memory, state, or custom logic in your app, the proxy does the heavy lifting, so your model responses feel coherent, continuous, and truly conversational.  

## Get the AlchemystAI API key.

Sign up and get you api key here : [Alchemyst platform](https://platform.getalchemystai.com/). Set your API key via environment variable or pass it directly when initializing the anthropic client.

- Recommended env var: `ALCHEMYST_AI_API_KEY`

```bash
export ALCHEMYST_AI_API_KEY="your_api_key_here"
```

## Quickstart

```ts 
import OpenAI from "openai";

const openaiURL = "https://api.openai.com/v1";

const client = new OpenAI({
  apiKey: process.env.ALCHEMYST_AI_API_KEY, 
  baseURL: `https://platform-backend.getalchemystai.com/api/v1/proxy/${openaiURL}/${process.env.OPENAI_API_KEY}`, 
});

async function main() {
  const completion = await client.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [
      { role: "system", content: "Help me with the questions." },
      { role: "user", content: "Hello, can you summarize how this proxy works?" },
    ],
  });

  console.log("Response from OpenAI via proxy:");
  console.log(completion.choices[0].message);
}

main().catch(console.error);


```